{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":205084,"sourceType":"modelInstanceVersion","modelInstanceId":72244,"modelId":78150}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma2: Fine-Tuning with LoRA on ChatDoctor-HealthCareMagic\n\nThis notebook demonstrates how to fine-tune Google's Gemma2-2B model using Parameter-Efficient Fine-Tuning (PEFT) with LoRA (Low-Rank Adaptation) on the [ChatDoctor-HealthCareMagic medical Q&A dataset](https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k).\n\n## To run:\nThis notebook was run on Kaggle using NVIDIA T4(x2) [GPT T4x2]. Kaggle offers T4 for free once you have verified your phone number.","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup Libraries\n\nInstall required packages and configure Keras backend for JAX with optimized memory usage on GPU.","metadata":{}},{"cell_type":"code","source":"!pip install -q -U keras-nlp\n!pip install -q -U \"keras>=3\"\n!pip install -q datasets\n!pip install -q pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:23:03.085906Z","iopub.execute_input":"2025-06-01T02:23:03.086273Z","iopub.status.idle":"2025-06-01T02:23:15.903363Z","shell.execute_reply.started":"2025-06-01T02:23:03.086244Z","shell.execute_reply":"2025-06-01T02:23:15.902366Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # Use JAX backend for Keras (optimized for GPU training)\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"  # Allocate 100% of GPU memory to JAX","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:23:15.904552Z","iopub.execute_input":"2025-06-01T02:23:15.904774Z","iopub.status.idle":"2025-06-01T02:23:15.908763Z","shell.execute_reply.started":"2025-06-01T02:23:15.904756Z","shell.execute_reply":"2025-06-01T02:23:15.908057Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import keras\nimport keras_nlp\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:23:15.910270Z","iopub.execute_input":"2025-06-01T02:23:15.910512Z","iopub.status.idle":"2025-06-01T02:23:19.906767Z","shell.execute_reply.started":"2025-06-01T02:23:15.910492Z","shell.execute_reply":"2025-06-01T02:23:19.906008Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## 2. Dataset Preparation\nLoad the ChatDoctor-HealthCareMagic dataset, sample, and split into training + test. Format data for instruction-following and prepare evaluation samples to compare model performance before and after fine-tuning.","metadata":{}},{"cell_type":"code","source":"NUM_TRAINING_INSTANCES = 1000 # Number of training examples to process. \nRANDOM_SEED = 42\nTRAIN_SPLIT = 0.9\nNUM_EVAL_SAMPLES = min(5, int(NUM_TRAINING_INSTANCES * (1 - TRAIN_SPLIT)))  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:23:19.907948Z","iopub.execute_input":"2025-06-01T02:23:19.908544Z","iopub.status.idle":"2025-06-01T02:23:19.912091Z","shell.execute_reply.started":"2025-06-01T02:23:19.908519Z","shell.execute_reply":"2025-06-01T02:23:19.911339Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"dataset = load_dataset(\"lavita/ChatDoctor-HealthCareMagic-100k\", split=\"train\")\ndataset = dataset.shuffle(seed=RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:23:19.913111Z","iopub.execute_input":"2025-06-01T02:23:19.913415Z","iopub.status.idle":"2025-06-01T02:23:21.206688Z","shell.execute_reply.started":"2025-06-01T02:23:19.913385Z","shell.execute_reply":"2025-06-01T02:23:21.205834Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"data = []\nfor example in dataset:\n    text = f\"Instruction:\\n{example['input']}\\n\\nResponse:\\n{example['output']}\"\n    data.append(text)\n    if len(data) >= NUM_TRAINING_INSTANCES:\n        break\n\n# Split 90% train, 10% test\nsplit_idx = int(len(data) * TRAIN_SPLIT)\ntrain_data = data[:split_idx]\ntest_data = data[split_idx:]\n\nprint(f\"Train: {len(train_data)} examples\")\nprint(f\"Test: {len(test_data)} examples\")\nprint(\"== Sample == \\n\", train_data[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:23:21.207659Z","iopub.execute_input":"2025-06-01T02:23:21.207935Z","iopub.status.idle":"2025-06-01T02:23:21.318410Z","shell.execute_reply.started":"2025-06-01T02:23:21.207912Z","shell.execute_reply":"2025-06-01T02:23:21.317559Z"}},"outputs":[{"name":"stdout","text":"Train: 900 examples\nTest: 100 examples\n== Sample == \n Instruction:\nI have been having alot of catching ,pain and discomfort under my right rib.  If I twist to either side especially my right it feels like my rib actually catches on something and at times I have to stop try to catch my breath and wait for it to subside.  There are times if I am laughing too hard that it will do the same thing but normally its more so if I have twisted or moved  a certain way\n\nResponse:\nHi thanks for asking question. Here you are complaining pain in particular position esp. While turning to a side. So strong possibility is about moderate degree muscular strain. It might have occurred by heavyweight lift or during some activities. Simple analgesic taken. Take rest. Sleep in supine position. Second here Costco Chat Doctor.  Ribs are tender to touch.x-ray also useful. If cough, cold, sore throat present then respiratory infections also has to be ruled out. Treat it symptomatically. If still seems serious then x-ray done for chest. CBC will also be done. If you have yellow sclera, right abdomen pain, anorexia then do your serum liver enzyme study for detecting liver pathology. Avoid stress as it can aggravate pain. I hope you will understand my concern.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import random\n\nrandom.seed(RANDOM_SEED)\n\neval_samples = random.sample(test_data, NUM_EVAL_SAMPLES)\n\n# Store evaluation data structure\neval_data = []\nfor i, test_sample in enumerate(eval_samples):\n    # Parse the test sample to extract question and answer\n    parts = test_sample.split(\"Instruction:\\n\")[1].split(\"\\n\\nResponse:\\n\")\n    question = parts[0].strip()\n    ground_truth_answer = parts[1].strip()\n    \n    eval_data.append({\n        'question': question,\n        'ground_truth': ground_truth_answer,\n        'before_finetuning': None,  # Will be filled in next section\n        'after_finetuning': None   # Will be filled after training\n    })\n\nprint(f\"Prepared {len(eval_data)} evaluation samples for comparison dataframe\")\nprint(f\"First evaluation question: {eval_data[0]['question'][:100]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:23:21.319284Z","iopub.execute_input":"2025-06-01T02:23:21.319580Z","iopub.status.idle":"2025-06-01T02:23:21.325275Z","shell.execute_reply.started":"2025-06-01T02:23:21.319549Z","shell.execute_reply":"2025-06-01T02:23:21.324635Z"}},"outputs":[{"name":"stdout","text":"Prepared 5 evaluation samples for comparison dataframe\nFirst evaluation question: i am female 50 yrs old i have urine infection and stabbing bladder pain for more then week now, and ...\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 3. Load Model & Baseline Evaluation\nLoad the Gemma 2B model and generate baseline responses on evaluation samples before any fine-tuning. This establishes the \"before\" performance for comparison.","metadata":{}},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\ngemma_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:23:21.327720Z","iopub.execute_input":"2025-06-01T02:23:21.327919Z","iopub.status.idle":"2025-06-01T02:24:10.109124Z","shell.execute_reply.started":"2025-06-01T02:23:21.327902Z","shell.execute_reply":"2025-06-01T02:24:10.108412Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import time\n\nprint(\"Generating responses on Gemma2-2b without fine-tuning...\")\nstart_time = time.time()\n\nfor i, eval_item in enumerate(eval_data):\n    sample_start = time.time()\n    # Use raw question for original model (no special formatting)\n    response = gemma_lm.generate(eval_item['question'], max_length=512)\n    eval_data[i]['before_finetuning'] = response.strip()\n    sample_time = time.time() - sample_start\n    print(f\"Processed sample {i+1}/{len(eval_data)} in {sample_time:.2f}s\")\n\ntotal_time = time.time() - start_time\navg_time = total_time / len(eval_data)\n\nprint(f\"\\n✅ Captured {len(eval_data)} before fine-tuning responses\")\nprint(f\"⏱️  Total inference time: {total_time:.2f}s\")\nprint(f\"⏱️  Average time per sample: {avg_time:.2f}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:24:10.110279Z","iopub.execute_input":"2025-06-01T02:24:10.110515Z","iopub.status.idle":"2025-06-01T02:26:03.840325Z","shell.execute_reply.started":"2025-06-01T02:24:10.110494Z","shell.execute_reply":"2025-06-01T02:26:03.839501Z"}},"outputs":[{"name":"stdout","text":"Generating responses on Gemma2-2b without fine-tuning...\nProcessed sample 1/5 in 14.91s\nProcessed sample 2/5 in 21.91s\nProcessed sample 3/5 in 26.62s\nProcessed sample 4/5 in 23.86s\nProcessed sample 5/5 in 26.42s\n\n✅ Captured 5 before fine-tuning responses\n⏱️  Total inference time: 113.72s\n⏱️  Average time per sample: 22.74s\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 4. PEFT using LoRA\n\nEnable Low-Rank Adaptation (LoRA) on the model backbone with rank 4, configure the optimizer, and fine-tune on the training data. LoRA allows efficient fine-tuning by only updating a small subset of parameters.","metadata":{}},{"cell_type":"code","source":"gemma_lm.backbone.enable_lora(rank=4)\ngemma_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:26:03.841118Z","iopub.execute_input":"2025-06-01T02:26:03.841360Z","iopub.status.idle":"2025-06-01T02:26:04.156127Z","shell.execute_reply.started":"2025-06-01T02:26:03.841340Z","shell.execute_reply":"2025-06-01T02:26:04.155317Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"optimizer = keras.optimizers.AdamW(\n    learning_rate=1e-4,\n    weight_decay=0.01,\n)\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.preprocessor.sequence_length = 256 # Limit input sequence\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:26:04.156862Z","iopub.execute_input":"2025-06-01T02:26:04.157117Z","iopub.status.idle":"2025-06-01T02:26:04.245566Z","shell.execute_reply.started":"2025-06-01T02:26:04.157096Z","shell.execute_reply":"2025-06-01T02:26:04.244867Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Fine-tune the model\nhistory = gemma_lm.fit(train_data, epochs=1, batch_size=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:48:03.702931Z","iopub.execute_input":"2025-06-01T02:48:03.703299Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m 55/900\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17:40\u001b[0m 1s/step - loss: 2.3238 - sparse_categorical_accuracy: 0.4242","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Print training metrics\nprint(\"Training History:\")\nprint(f\"Available metrics: {list(history.history.keys())}\")\nprint(\"\\nTraining Progress:\")\nfor epoch in range(len(history.history['loss'])):\n    print(f\"Epoch {epoch + 1}:\")\n    for metric, values in history.history.items():\n        print(f\"  {metric}: {values[epoch]:.4f}\")\n    print()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Inference Pipeline & Evaluation\n\nConfigure the fine-tuned model with Top-K sampling, generate responses on the same evaluation questions, and create a comparison DataFrame showing before vs. after fine-tuning performance.","metadata":{}},{"cell_type":"code","source":"# Compile the model with the strategy again\nstrategy = keras_nlp.samplers.TopKSampler(k=10, temperature=0.7, seed=42)  \ngemma_lm.compile(sampler=strategy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_prompt(user_question):\n    return f\"Instruction:\\n{user_question}\\n\\nResponse:\\n\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Generating after fine-tuning responses...\")\nfor i, eval_item in enumerate(eval_data):\n    formatted_prompt = format_prompt(eval_item['question'])\n    response = gemma_lm.generate(formatted_prompt, max_length=200)\n    clean_response = response.replace(formatted_prompt, \"\").strip()\n    eval_data[i]['after_finetuning'] = clean_response\n\nprint(f\"Captured {len(eval_data)} after fine-tuning responses\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Convert evaluation data directly to DataFrame and save\ncomparison_df = pd.DataFrame(eval_data)\n\nprint(f\"✅ DataFrame created with {len(comparison_df)} samples\")\nprint(f\"Columns: {list(comparison_df.columns)}\")\n\n# Display sample results\nprint(\"\\n== SAMPLE COMPARISON ==\")\nfor i in range(min(2, len(comparison_df))):\n    print(f\"Question: {comparison_df.iloc[i]['question']}\")\n    print(f\"Ground Truth: {comparison_df.iloc[i]['ground_truth']}\")\n    print(f\"Before Fine-tuning: {comparison_df.iloc[i]['before_finetuning']}\")\n    print(f\"After Fine-tuning: {comparison_df.iloc[i]['after_finetuning']}\")\n    print(\"-------\\n\")\n\n# Save to CSV\ncomparison_df.to_csv('gemma2_2b_comparison.csv', index=False)\nprint(\"💾 Results saved to 'gemma2_2b_comparison.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}