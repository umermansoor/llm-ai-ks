{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma2: Fine-Tuning with LoRA on ChatDoctor-HealthCareMagic\n",
    "\n",
    "This notebook demonstrates how to fine-tune Google's Gemma2-2B model using Parameter-Efficient Fine-Tuning (PEFT) with LoRA (Low-Rank Adaptation) on the [ChatDoctor-HealthCareMagic medical Q&A dataset](https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k).\n",
    "\n",
    "## To run:\n",
    "This notebook was run on Kaggle using NVIDIA T4(x2) [GPT T4x2]. Kaggle offers T4 for free once you have verified your phone number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Libraries\n",
    "\n",
    "Install required packages and configure Keras backend for JAX with optimized memory usage on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-01T01:18:32.259104Z",
     "iopub.status.busy": "2025-06-01T01:18:32.258760Z",
     "iopub.status.idle": "2025-06-01T01:18:52.785748Z",
     "shell.execute_reply": "2025-06-01T01:18:52.784820Z",
     "shell.execute_reply.started": "2025-06-01T01:18:32.259070Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m873.6/873.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U \"keras>=3\"\n",
    "!pip install -q datasets\n",
    "!pip install -q pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T01:18:52.786901Z",
     "iopub.status.busy": "2025-06-01T01:18:52.786686Z",
     "iopub.status.idle": "2025-06-01T01:18:52.790884Z",
     "shell.execute_reply": "2025-06-01T01:18:52.790038Z",
     "shell.execute_reply.started": "2025-06-01T01:18:52.786882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  \n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T01:18:52.791992Z",
     "iopub.status.busy": "2025-06-01T01:18:52.791734Z",
     "iopub.status.idle": "2025-06-01T01:19:09.241881Z",
     "shell.execute_reply": "2025-06-01T01:19:09.241227Z",
     "shell.execute_reply.started": "2025-06-01T01:18:52.791943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "Load the ChatDoctor-HealthCareMagic dataset, sample 1000 examples, and split into training + test samples. Format data for instruction-following and prepare evaluation samples to compare model performance before and after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T01:19:09.244064Z",
     "iopub.status.busy": "2025-06-01T01:19:09.243549Z",
     "iopub.status.idle": "2025-06-01T01:19:09.247396Z",
     "shell.execute_reply": "2025-06-01T01:19:09.246656Z",
     "shell.execute_reply.started": "2025-06-01T01:19:09.244041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_TRAINING_INSTANCES = 1000 # Number of samples to process. \n",
    "RANDOM_SEED = 42\n",
    "TRAIN_SPLIT = 0.9\n",
    "NUM_EVAL_SAMPLES = min(5, int(NUM_TRAINING_INSTANCES * (1 - TRAIN_SPLIT)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T01:19:09.248704Z",
     "iopub.status.busy": "2025-06-01T01:19:09.248477Z",
     "iopub.status.idle": "2025-06-01T01:19:11.759361Z",
     "shell.execute_reply": "2025-06-01T01:19:11.758316Z",
     "shell.execute_reply.started": "2025-06-01T01:19:09.248672Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253ab19d6cc7485bbd252ed6b062b3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/542 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996e9ec1a95745159162da94edf49212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-5e7cb295b9cff0bf.parquet:   0%|          | 0.00/70.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a5fc7e26224914ada00b41337d2f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/112165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"lavita/ChatDoctor-HealthCareMagic-100k\", split=\"train\")\n",
    "dataset = dataset.shuffle(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T01:19:11.760810Z",
     "iopub.status.busy": "2025-06-01T01:19:11.760469Z",
     "iopub.status.idle": "2025-06-01T01:19:11.856590Z",
     "shell.execute_reply": "2025-06-01T01:19:11.855662Z",
     "shell.execute_reply.started": "2025-06-01T01:19:11.760780Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 900 examples\n",
      "Test: 100 examples\n",
      "== Sample == \n",
      " Instruction:\n",
      "I have been having alot of catching ,pain and discomfort under my right rib.  If I twist to either side especially my right it feels like my rib actually catches on something and at times I have to stop try to catch my breath and wait for it to subside.  There are times if I am laughing too hard that it will do the same thing but normally its more so if I have twisted or moved  a certain way\n",
      "\n",
      "Response:\n",
      "Hi thanks for asking question. Here you are complaining pain in particular position esp. While turning to a side. So strong possibility is about moderate degree muscular strain. It might have occurred by heavyweight lift or during some activities. Simple analgesic taken. Take rest. Sleep in supine position. Second here Costco Chat Doctor.  Ribs are tender to touch.x-ray also useful. If cough, cold, sore throat present then respiratory infections also has to be ruled out. Treat it symptomatically. If still seems serious then x-ray done for chest. CBC will also be done. If you have yellow sclera, right abdomen pain, anorexia then do your serum liver enzyme study for detecting liver pathology. Avoid stress as it can aggravate pain. I hope you will understand my concern.\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for example in dataset:\n",
    "    text = f\"Instruction:\\n{example['input']}\\n\\nResponse:\\n{example['output']}\"\n",
    "    data.append(text)\n",
    "    if len(data) >= NUM_TRAINING_INSTANCES:\n",
    "        break\n",
    "\n",
    "# Split 90% train, 10% test\n",
    "split_idx = int(len(data) * TRAIN_SPLIT)\n",
    "train_data = data[:split_idx]\n",
    "test_data = data[split_idx:]\n",
    "\n",
    "print(f\"Train: {len(train_data)} examples\")\n",
    "print(f\"Test: {len(test_data)} examples\")\n",
    "print(\"== Sample == \\n\", train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T01:19:11.857724Z",
     "iopub.status.busy": "2025-06-01T01:19:11.857495Z",
     "iopub.status.idle": "2025-06-01T01:19:11.863646Z",
     "shell.execute_reply": "2025-06-01T01:19:11.862629Z",
     "shell.execute_reply.started": "2025-06-01T01:19:11.857704Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 100 evaluation samples for comparison dataframe\n",
      "First evaluation question: i am female 50 yrs old i have urine infection and stabbing bladder pain for more then week now, and ...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "eval_samples = random.sample(test_data, NUM_EVAL_SAMPLES)\n",
    "\n",
    "# Store evaluation data structure\n",
    "eval_data = []\n",
    "for i, test_sample in enumerate(eval_samples):\n",
    "    # Parse the test sample to extract question and answer\n",
    "    parts = test_sample.split(\"Instruction:\\n\")[1].split(\"\\n\\nResponse:\\n\")\n",
    "    question = parts[0].strip()\n",
    "    ground_truth_answer = parts[1].strip()\n",
    "    \n",
    "    eval_data.append({\n",
    "        'question': question,\n",
    "        'ground_truth': ground_truth_answer,\n",
    "        'before_finetuning': None,  # Will be filled in next section\n",
    "        'after_finetuning': None   # Will be filled after training\n",
    "    })\n",
    "\n",
    "print(f\"Prepared {len(eval_data)} evaluation samples for comparison dataframe\")\n",
    "print(f\"First evaluation question: {eval_data[0]['question'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model & Baseline Evaluation\n",
    "Load the Gemma 2B model and generate baseline responses on evaluation samples before any fine-tuning. This establishes the \"before\" performance for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T01:19:11.864851Z",
     "iopub.status.busy": "2025-06-01T01:19:11.864503Z",
     "iopub.status.idle": "2025-06-01T01:20:03.007244Z",
     "shell.execute_reply": "2025-06-01T01:20:03.006562Z",
     "shell.execute_reply.started": "2025-06-01T01:19:11.864820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T01:20:03.008364Z",
     "iopub.status.busy": "2025-06-01T01:20:03.008051Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE FINE-TUNING\n",
      "Generating before fine-tuning responses...\n"
     ]
    }
   ],
   "source": [
    "print(\"BEFORE FINE-TUNING\")\n",
    "\n",
    "print(\"Generating before fine-tuning responses...\")\n",
    "for i, eval_item in enumerate(eval_data):\n",
    "    # Use raw question for original model (no special formatting)\n",
    "    response = gemma_lm.generate(eval_item['question'], max_length=512)\n",
    "    eval_data[i]['before_finetuning'] = response.strip()\n",
    "\n",
    "print(f\"Captured {len(eval_data)} before fine-tuning responses\")\n",
    "\n",
    "# Define format function for after fine-tuning (when model expects this format)\n",
    "def format_prompt(user_question):\n",
    "    return f\"Instruction:\\n{user_question}\\n\\nResponse:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PEFT using LoRA\n",
    "\n",
    "Enable Low-Rank Adaptation (LoRA) on the model backbone with rank 4, configure the optimizer, and fine-tune on the training data. LoRA allows efficient fine-tuning by only updating a small subset of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "gemma_lm.preprocessor.sequence_length = 256 # Limit input sequence\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "history = gemma_lm.fit(train_data, epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print training metrics\n",
    "print(\"Training History:\")\n",
    "print(f\"Available metrics: {list(history.history.keys())}\")\n",
    "print(\"\\nTraining Progress:\")\n",
    "for epoch in range(len(history.history['loss'])):\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    for metric, values in history.history.items():\n",
    "        print(f\"  {metric}: {values[epoch]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference Pipeline & Evaluation\n",
    "\n",
    "Configure the fine-tuned model with Top-K sampling, generate responses on the same evaluation questions, and create a comparison DataFrame showing before vs. after fine-tuning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compile the model with the strategy again\n",
    "strategy = keras_nlp.samplers.TopKSampler(k=10, temperature=0.7, seed=42)  \n",
    "gemma_lm.compile(sampler=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Generating after fine-tuning responses...\")\n",
    "for i, eval_item in enumerate(eval_data):\n",
    "    formatted_prompt = format_prompt(eval_item['question'])\n",
    "    response = gemma_lm.generate(formatted_prompt, max_length=200)\n",
    "    clean_response = response.replace(formatted_prompt, \"\").strip()\n",
    "    eval_data[i]['after_finetuning'] = clean_response\n",
    "\n",
    "print(f\"Captured {len(eval_data)} after fine-tuning responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from the collected evaluation data\n",
    "comparison_data = []\n",
    "for eval_item in eval_data:\n",
    "    comparison_data.append({\n",
    "        'Question': eval_item['question'],\n",
    "        'Ground_Truth_Answer': eval_item['ground_truth'],\n",
    "        'Gemma2_2B_Before_FineTuning': eval_item['before_finetuning'],\n",
    "        'Gemma2_2B_After_FineTuning': eval_item['after_finetuning']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(f\"DataFrame created with {len(comparison_df)} samples\")\n",
    "print(f\"Columns: {list(comparison_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"== SAMPLE DATA ==\")\n",
    "for eval_item in eval_data[:2]:\n",
    "    print(\"Question: \", eval_item['question'])\n",
    "    print(\"Ground Truth Answer: \", eval_item['ground_truth'])\n",
    "    print(\"Gemma2_2B: \", eval_item['before_finetuning'])\n",
    "    print(\"Gemma2_2B-PEFT: \", eval_item['after_finetuning'])\n",
    "    print(\"-------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the dataframe to CSV for further analysis\n",
    "comparison_df.to_csv('gemma2b_finetuning_comparison.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "modelId": 78150,
     "modelInstanceId": 72244,
     "sourceId": 85984,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 78150,
     "modelInstanceId": 72244,
     "sourceId": 205084,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
